{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3815ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084a1e6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (2691046317.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[120]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31minference_results_test_silico.ste.6.png_0_1000.csv\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "file_paths = [\n",
    "    #\"inference_results_train_silico.ptq.6.png_0_5000.csv\",\n",
    "    #\"inference_results_train_silico.ptq.6.png_5000_5000.csv\"\n",
    "    #\"inference_results_test_silico.ptq.6.png_0_1000.csv\"\n",
    "    \"inference_results_test_silico.ste.6.png_0_1000.csv\"\n",
    "]\n",
    "\n",
    "# Combine data from multiple CSV files\n",
    "data_frames = []\n",
    "for file_path in file_paths:\n",
    "    df = pd.read_csv(file_path)\n",
    "    data_frames.append(df)\n",
    "\n",
    "data = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "confusion_matrix = np.zeros((10, 10), dtype=int)\n",
    "for index, row in data.iterrows():\n",
    "    true_label = int(row['Label'])\n",
    "    predicted_label = int(row['Prediction'])\n",
    "    confusion_matrix[true_label][predicted_label] += 1\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Reds)\n",
    "# Overlay counts on the heatmap\n",
    "thresh = confusion_matrix.max() / 2.\n",
    "for i in range(confusion_matrix.shape[0]):\n",
    "    for j in range(confusion_matrix.shape[1]):\n",
    "        plt.text(j, i, format(confusion_matrix[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if confusion_matrix[i, j] > thresh else \"black\")\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(10)\n",
    "plt.xticks(tick_marks, range(10))\n",
    "plt.yticks(tick_marks, range(10))\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "54bbb5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0: 451 samples\n",
      "Label 1: 553 samples\n",
      "Label 2: 495 samples\n",
      "Label 3: 530 samples\n",
      "Label 4: 501 samples\n",
      "Label 5: 449 samples\n",
      "Label 6: 460 samples\n",
      "Label 7: 531 samples\n",
      "Label 8: 508 samples\n",
      "Label 9: 522 samples\n"
     ]
    }
   ],
   "source": [
    "# Give the count of labels per class\n",
    "label_counts = data['Label'].value_counts().sort_index()\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"Label {label}: {count} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "76f1b27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 65.77%\n",
      "Accuracy: 65.77%\n"
     ]
    }
   ],
   "source": [
    "# From confusion matrix, calculate accuracy\n",
    "correct_predictions = np.trace(confusion_matrix)\n",
    "total_predictions = np.sum(confusion_matrix)\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "# It saves correct in pd\n",
    "accuracy = data['Correct'].sum() / len(data)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df38615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efdb6487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Reward for label 0: -0.98\n",
      "Average Reward for label 7: -1.00\n",
      "Average Reward for label 1: -1.10\n",
      "Average Reward for label 2: -1.18\n",
      "Average Reward for label 8: -1.26\n",
      "Average Reward for label 6: -1.28\n",
      "Average Reward for label 4: -1.29\n",
      "Average Reward for label 3: -1.66\n",
      "Average Reward for label 9: -1.80\n",
      "Average Reward for label 5: -1.95\n"
     ]
    }
   ],
   "source": [
    "# Print the average Reward for each label, print from largest to smallest\n",
    "average_rewards = []\n",
    "for label in range(10):\n",
    "    label_data = data[data['Label'] == label]\n",
    "    average_reward = label_data['Reward'].mean()\n",
    "    average_rewards.append((label, average_reward))\n",
    "\n",
    "# Sort by average reward\n",
    "average_rewards.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for label, avg_reward in average_rewards:\n",
    "    print(f'Average Reward for label {label}: {avg_reward:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
